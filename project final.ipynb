{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26e49c7-4d5a-4f82-b256-7766908c4e54",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6eaf18-8e0a-45dc-a3e1-ae13b92a92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1840e5-21ea-4b72-be31-775eb88595ef",
   "metadata": {},
   "source": [
    "Read data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5a50d-c5b9-4dff-8996-688f5b19f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"synthetic_network_traffic.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e52e0f-bfe5-4708-bf0d-ff62bc716eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffb935-2dc5-4080-b8a5-c1e875eca416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3e352-9083-4f89-854d-8882e27b7ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadeb913-c00e-4023-8b38-73bb5476f147",
   "metadata": {},
   "source": [
    "Check the total number of null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990d4a3-a1a7-4496-9f02-61a721553c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510d38d-78e2-467b-9b3a-91be42cf8003",
   "metadata": {},
   "source": [
    "Checking the values of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313d9ed-450a-4a4f-bece-ae25141ef221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SourceIP.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f7082-b381-4bef-b147-f7720ab52d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DestinationIP.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568152f-0ce5-405b-82ad-c224777286cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DestinationPort.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bc8ac-9f13-4dab-b93e-0414db30c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Protocol.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fbb71e-1fa6-417b-92e7-f4f5bbf79372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BytesSent.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b2faf-c58f-4521-b749-b684c0cb1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BytesReceived.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1eab7-2896-4ce1-90f7-56e97dcad834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PacketsSent.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe9581-fbb7-48b7-8c8d-c3bfb0f7136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PacketsReceived.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5065469-f2b0-4211-83c0-91b1740b0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Duration.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3dcd9-7de4-4fd7-bc86-17b4f9db6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.IsAnomaly.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d1cb9-0228-41ae-aafb-9277d044c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PacketsSent\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7cf86a-4842-48fc-8e51-bb309516a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PacketsReceived\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99715b7f-d7bd-4e0a-9eb3-2432dfffa549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eafa74-4e07-451f-a26a-e0d4ea71fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just want to check the correlation between each column, to understand what features are corelated\n",
    "import seaborn as sns\n",
    "correlation_matrix = df.corr()\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "sns.heatmap(correlation_matrix,vmax=1.0,square = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e9477-51ae-4108-880e-09d12da0addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Generate additional features (you can customize this)\n",
    "df['TotalBytes'] = df['BytesSent'] + df['BytesReceived']\n",
    "df['TotalPackets'] = df['PacketsSent'] + df['PacketsReceived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edb24b-ebf8-43c2-b41e-177076ae5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the 'Anomaly' class to balance the class distribution\n",
    "anomaly_data = df[df['IsAnomaly'] == 1]\n",
    "oversampled_data = pd.concat([df, anomaly_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f3fcc-2aae-4d12-8e11-9bfb23d8dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_fraction = np.ceil(len(anomaly_data)/float(len(oversampled_data)))\n",
    "\n",
    "#Let's print how many more outliers are there in the dataset compared to normal data\n",
    "print(outlier_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca4aff-e416-45fd-aef7-fa2f5b9a109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and labels\n",
    "X = oversampled_data.drop(columns=['IsAnomaly'])  # Features\n",
    "y = oversampled_data['IsAnomaly']  # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f05312-d3c5-420f-8871-deab76cf22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebabd6b-25bc-4498-aef0-df23ef098dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6d8be-8591-44b8-83ef-63a9ea5a4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random state \n",
    "state = np.random.RandomState(42)\n",
    "#Let's initalize some classifiers\n",
    "classifiers = {\n",
    "    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X_train), \n",
    "                                       contamination=outlier_fraction,random_state=state, verbose=1),\n",
    "    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n",
    "                                              leaf_size=30, metric='minkowski',\n",
    "                                              p=2, metric_params=None, contamination=0.03),\n",
    "    \"Support Vector Machine\":OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n",
    "                                         max_iter=-1 )\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d604ca2-124e-48a8-8a8f-b1e84fb42980",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outliers = len(anomaly_data)\n",
    "for i, (clf_name,clf) in enumerate(classifiers.items()):\n",
    "    #Fit the data and tag outliers\n",
    "    if clf_name == \"Local Outlier Factor\":\n",
    "        continue\n",
    "        y_pred = clf.fit_predict(X_train)\n",
    "        scores_prediction = clf.negative_outlier_factor_\n",
    "    elif clf_name == \"Support Vector Machine\":\n",
    "        clf.fit(X_train)\n",
    "        y_pred = clf.predict(X_train)\n",
    "    else:    \n",
    "        continue\n",
    "        clf.fit(X_train)\n",
    "        scores_prediction = clf.decision_function(X_train)\n",
    "        y_pred = clf.predict(X_train)\n",
    "        \n",
    "    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n",
    "    y_pred[y_pred == 1] = 0\n",
    "    y_pred[y_pred == -1] = 1\n",
    "    n_errors = (y_pred != y_train).sum()\n",
    "    # Run Classification Metrics\n",
    "    print(\"{}: {}\".format(clf_name,n_errors))\n",
    "    print(\"Accuracy Score :\")\n",
    "    print(accuracy_score(y_train,y_pred))\n",
    "    print(\"Classification Report :\")\n",
    "    print(classification_report(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87072801-ace9-4b85-9a2c-45f6cca4af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outliers = len(anomaly_data)\n",
    "for i, (clf_name,clf) in enumerate(classifiers.items()):\n",
    "    #Fit the data and tag outliers\n",
    "    if clf_name == \"Local Outlier Factor\":\n",
    "        continue\n",
    "        y_pred = clf.fit_predict(X_train)\n",
    "        scores_prediction = clf.negative_outlier_factor_\n",
    "    elif clf_name == \"Support Vector Machine\":\n",
    "        clf.fit(X_train)\n",
    "        y_pred = clf.predict(X_train)\n",
    "    else:    \n",
    "        continue\n",
    "        clf.fit(X_train)\n",
    "        scores_prediction = clf.decision_function(X_train)\n",
    "        y_pred = clf.predict(X_train)\n",
    "        \n",
    "    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n",
    "    y_pred[y_pred == 1] = 0\n",
    "    y_pred[y_pred == -1] = 1\n",
    "    n_errors = (y_pred != y_train).sum()\n",
    "    # Run Classification Metrics\n",
    "    print(\"{}: {}\".format(clf_name,n_errors))\n",
    "    print(\"Accuracy Score :\")\n",
    "    print(accuracy_score(y_train,y_pred))\n",
    "    print(\"Classification Report :\")\n",
    "    print(classification_report(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1120c-7bbf-4ade-9991-3ef3a89e30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55732b27-44f9-4027-9374-0b258425bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(x1, y1, name=\"graph.png\"):\n",
    "    \n",
    "    #Scale features to improve the training ability of TSNE.\n",
    "    standard_scaler = StandardScaler()\n",
    "    df2_std = standard_scaler.fit_transform(x1)\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    x_test_2d = tsne.fit_transform(df2_std)\n",
    "    \n",
    "    #Build the scatter plot with the two types of transactions.\n",
    "    color_map = {0:'green', 1:'red'}\n",
    "    plt.figure()\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x = x_test_2d[y1==cl,0], \n",
    "                    y = x_test_2d[y1==cl,1], \n",
    "                    c = color_map[idx], \n",
    "                    label = cl)\n",
    "    plt.xlabel('X in t-SNE')\n",
    "    plt.ylabel('Y in t-SNE')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('t-SNE visualization of test data')\n",
    "    plt.show()\n",
    "    \n",
    "tsne_plot(X_train[:8000], Y_train[:8000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf51452-a944-43ed-b9b0-454e0f796400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think training this kind of dataset with autoencoders makes more sense so let's try that\n",
    "\n",
    "## input layer \n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "## encoding part\n",
    "encoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoded = Dense(50, activation='relu')(encoded)\n",
    "\n",
    "## decoding part\n",
    "decoded = Dense(50, activation='tanh')(encoded)\n",
    "decoded = Dense(100, activation='tanh')(decoded)\n",
    "\n",
    "## output layer\n",
    "output_layer = Dense(X_train.shape[1], activation='relu')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa7387-1b98-4b73-b36f-9b9223e8ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the mdoel\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231accf-96ea-4bd1-9c84-146e2d3829ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do bit of data transformation for scaling\n",
    "\n",
    "x = X_train\n",
    "y = Y_train\n",
    "\n",
    "x_scale = preprocessing.MinMaxScaler().fit_transform(x.values)\n",
    "x_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f71422-087b-459a-acbb-12b6fa544f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_norm[0:8000], x_norm[0:8000], \n",
    "                batch_size = 256, epochs = 50, \n",
    "                shuffle = True, validation_split = 0.20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c28a64-65dc-4046-8d72-d6cd6f60744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally we can train a classifier on learnt representations\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\n",
    "clf = LogisticRegression(solver=\"lbfgs\").fit(X_train, Y_train)\n",
    "pred_y = clf.predict(X_val)\n",
    "\n",
    "print (\"\")\n",
    "print (\"Classification Report: \")\n",
    "print (classification_report(Y_val, pred_y))\n",
    "\n",
    "print (\"\")\n",
    "print (\"Accuracy Score: \", accuracy_score(Y_val, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa978f-ff1d-4c8a-9073-8218c4c928cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\chevu\\uopi\\synthetic_network_traffic.csv\"  # Update your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Initialize and train Isolation Forest\n",
    "model = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
    "model.fit(data)\n",
    "\n",
    "# Predict anomalies\n",
    "data['anomaly'] = model.predict(data)\n",
    "data['anomaly'] = data['anomaly'].apply(lambda x: 1 if x == -1 else 0)  # -1 indicates anomaly\n",
    "\n",
    "# Display results\n",
    "print(data['anomaly'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b8775-2c41-438b-af69-e4c8eefd30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.IsAnomaly.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
